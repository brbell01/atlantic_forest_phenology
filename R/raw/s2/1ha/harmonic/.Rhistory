S2_band <- "EVI2" #
analysis_start <- ymd("2019-01-01")
names <- dir(wd, full.names = T, pattern = paste0(S2_band, "_2016_2022_harmonic.csv"))
tables <- names %>% map(read.csv, header=T)
# 1) R-SQUARED error metric -- Coefficient of Determination
RSQUARE <- function(y_actual,y_pred){
cor(y_actual,y_pred)^2
}
# 2) MEAN ABSOLUTE PERCENTAGE ERROR (MAPE)
MAPE <- function(y_actual,y_pred){
mean(abs((y_actual-y_pred)/y_actual))*100
}
#define outlier function
remove_outliers <- function(x, na.rm = TRUE, ...) {
qnt <- quantile(x, probs=c(0.1, 0.9), na.rm = na.rm, ...)
H <- 1.5 * IQR(x, na.rm = na.rm)
y <- x
y[x < (qnt[1] - H)] <- NA
y[x > (qnt[2] + H)] <- NA
y
}
# all years
alltable <- data.frame(matrix(NA, nrow = 2, ncol = 1))
for (i in 1:length(tables)) {
names(tables[[i]])[1] <- "Date"
tables[[i]] <- tables[[i]] %>% na.omit()
# no_out <- tables[[i]][[2]] %>% remove_outliers()
# tables[[i]] <- tables[[i]][tables[[i]][[2]] %in% no_out, ]
tables[[i]] <- tables[[i]] %>% group_by(Date) %>% summarise_all(mean)  # check for duplicate data for the same day.
tables[[i]]$Date <- as.Date(tables[[i]]$Date, format = "%b %d, %Y")
tables[[i]] <- tables[[i]][Date >= analysis_start]
actual <- tables[[i]][2] %>% unlist %>% as.numeric()
pred <- tables[[i]][3] %>% unlist %>% as.numeric()
# 1 calculate R2
R2 <- RSQUARE(actual,pred)
# 2 calculate MAPE
Mape <- MAPE(actual,pred)
#write out processed file
filename_base <- names[[i]] %>% substring(1,nchar(names[[i]])-4)
summary <- c("R2"=R2, "MAPE"=Mape) %>% data.frame()
names(summary) <- c(as.vector(names[[i]]))
alltable <- cbind(alltable, summary)
# write.csv(summary, paste0(filename_base, "_R2.csv"))
}
library(stringi)
library(data.table)
library(tidyr)
library(purrr)
library(lubridate)
library(tidyquant)
#Report the R2 statistic for a given pair of fitted and raw values
#working directory
setwd("/Users/brbell01/Google Drive/PHD/CUNY/Research/R/Phenology/raw/s2/1ha/harmonic")
wd <- getwd()
S2_band <- "EVI2" #
analysis_start <- ymd("2019-01-01")
names <- dir(wd, full.names = T, pattern = paste0(S2_band, "_2016_2022_harmonic.csv"))
tables <- names %>% map(read.csv, header=T)
# 1) R-SQUARED error metric -- Coefficient of Determination
RSQUARE <- function(y_actual,y_pred){
cor(y_actual,y_pred)^2
}
# 2) MEAN ABSOLUTE PERCENTAGE ERROR (MAPE)
MAPE <- function(y_actual,y_pred){
mean(abs((y_actual-y_pred)/y_actual))*100
}
#define outlier function
remove_outliers <- function(x, na.rm = TRUE, ...) {
qnt <- quantile(x, probs=c(0.1, 0.9), na.rm = na.rm, ...)
H <- 1.5 * IQR(x, na.rm = na.rm)
y <- x
y[x < (qnt[1] - H)] <- NA
y[x > (qnt[2] + H)] <- NA
y
}
# all years
alltable <- data.frame(matrix(NA, nrow = 2, ncol = 1))
for (i in 1:length(tables)) {
names(tables[[i]])[1] <- "Date"
tables[[i]] <- tables[[i]] %>% na.omit()
# no_out <- tables[[i]][[2]] %>% remove_outliers()
# tables[[i]] <- tables[[i]][tables[[i]][[2]] %in% no_out, ]
tables[[i]] <- tables[[i]] %>% group_by(Date) %>% summarise_all(mean)  # check for duplicate data for the same day.
tables[[i]]$Date <- as.Date(tables[[i]]$Date, format = "%b %d, %Y")
tables[[i]] <- tables[[i]] %>% filter(Date > analysis_start)
actual <- tables[[i]][2] %>% unlist %>% as.numeric()
pred <- tables[[i]][3] %>% unlist %>% as.numeric()
# 1 calculate R2
R2 <- RSQUARE(actual,pred)
# 2 calculate MAPE
Mape <- MAPE(actual,pred)
#write out processed file
filename_base <- names[[i]] %>% substring(1,nchar(names[[i]])-4)
summary <- c("R2"=R2, "MAPE"=Mape) %>% data.frame()
names(summary) <- c(as.vector(names[[i]]))
alltable <- cbind(alltable, summary)
# write.csv(summary, paste0(filename_base, "_R2.csv"))
}
#cleanup
alltable <- alltable[, -c(1)]
names(alltable) <- names(alltable) %>% substring(1,nchar(.)-23) %>% stri_replace_all_fixed(., " ", "")
cutposition <- names(alltable)[1] %>% gregexpr('/',.) %>% unlist() %>% tail(n=1)
names(alltable) <- sub('_s2_1ha_', '_', names(alltable)) %>% substring(cutposition+1, nchar(.))
alltable <- alltable[,order(colnames(alltable))]
# year-by-year
years <- tables[[1]]$Date %>% as.Date(tables[[i]]$Date, format = "%b %d, %Y") %>% year() %>% unique()
year <- years[years>2018]
summary_table <- vector(mode = "list", length = length(tables))
names(summary_table) <- names(alltable)
for (i in 1:length(tables)) {
annualtable <- data.frame(year = years, R2 = rep(0, length(years)), Mape = rep(0, length(years)))
names(tables[[i]])[1] <- "Date"
tables[[i]]$Date <- as.Date(tables[[i]]$Date, format = "%b %d, %Y")
tables[[i]] <- tables[[i]] %>% filter(Date > analysis_start)
tables[[i]] <- tables[[i]] %>% na.omit()
# no_out <- tables[[i]][[2]] %>% remove_outliers()
# tables[[i]] <- tables[[i]][tables[[i]][[2]] %in% no_out, ]
tables[[i]] <- tables[[i]] %>% group_by(Date) %>% summarise_all(mean)  # check for duplicate data for the same day
for (year in years){ # change this loop to iterate over years
yeardata <- tables[[i]] %>% filter(year(Date) == year)
actual <- yeardata[[2]] %>% unlist %>% as.numeric()
pred <- yeardata[[3]] %>% unlist %>% as.numeric()
# Calculate R2
R2 <- RSQUARE(actual, pred)
# Calculate MAPE
Mape <- MAPE(actual, pred)
# Write out processed file
filename_base <- substring(names(tables[i]), 1, nchar(names(tables[i])) - 4)
annual <- data.frame(year,R2,Mape)
annualtable <- rows_update(annualtable, annual)
}
# Add to yearly totals in the summary table
summary_table[[i]] <- annualtable
}
#write out summary file
#write.csv(alltable, paste0(wd, "S2_", S2_band, "_harmonic_model_fit_stats.csv"))
# #write out data files
# write.csv(tables[[1]], paste0(wd, "/mata_escura_S2_", S2_band, "_harmonic_model_data.csv"))
# write.csv(tables[[2]], paste0(wd, "/monte_pascoal_S2_", S2_band, "_harmonic_model_data.csv"))
# write.csv(tables[[3]], paste0(wd, "/rio_doce_S2_", S2_band, "_harmonic_model_data.csv"))
# write.csv(tables[[4]], paste0(wd, "/sooretama_S2_", S2_band, "_harmonic_model_data.csv"))
#write out annual data files
write.csv(summary_table[[1]], paste0(wd, "/mata_escura_S2_", S2_band, "_harmonic_model_annual_fit_stats.csv"))
write.csv(summary_table[[2]], paste0(wd, "/monte_pascoal_S2_", S2_band, "_harmonic_model_annual_fit_stats.csv"))
write.csv(summary_table[[3]], paste0(wd, "/rio_doce_S2_", S2_band, "_harmonic_model_annual_fit_stats.csv"))
write.csv(summary_table[[4]], paste0(wd, "/sooretama_S2_", S2_band, "_harmonic_model_annual_fit_stats.csv"))
library(stringi)
library(data.table)
library(tidyr)
library(purrr)
library(lubridate)
library(tidyquant)
#Report the R2 statistic for a given pair of fitted and raw values
#working directory
setwd("/Users/brbell01/Google Drive/PHD/CUNY/Research/R/Phenology/raw/s2/1ha/harmonic")
wd <- getwd()
S2_band <- "EVI2" #
analysis_start <- ymd("2019-01-01")
names <- dir(wd, full.names = T, pattern = paste0(S2_band, "_2016_2022_harmonic.csv"))
tables <- names %>% map(read.csv, header=T)
# 1) R-SQUARED error metric -- Coefficient of Determination
RSQUARE <- function(y_actual,y_pred){
cor(y_actual,y_pred)^2
}
# 2) MEAN ABSOLUTE PERCENTAGE ERROR (MAPE)
MAPE <- function(y_actual,y_pred){
mean(abs((y_actual-y_pred)/y_actual))*100
}
#define outlier function
remove_outliers <- function(x, na.rm = TRUE, ...) {
qnt <- quantile(x, probs=c(0.1, 0.9), na.rm = na.rm, ...)
H <- 1.5 * IQR(x, na.rm = na.rm)
y <- x
y[x < (qnt[1] - H)] <- NA
y[x > (qnt[2] + H)] <- NA
y
}
# all years
alltable <- data.frame(matrix(NA, nrow = 2, ncol = 1))
for (i in 1:length(tables)) {
names(tables[[i]])[1] <- "Date"
tables[[i]] <- tables[[i]] %>% na.omit()
# no_out <- tables[[i]][[2]] %>% remove_outliers()
# tables[[i]] <- tables[[i]][tables[[i]][[2]] %in% no_out, ]
tables[[i]] <- tables[[i]] %>% group_by(Date) %>% summarise_all(mean)  # check for duplicate data for the same day.
tables[[i]]$Date <- as.Date(tables[[i]]$Date, format = "%b %d, %Y")
tables[[i]] <- tables[[i]] %>% filter(Date > analysis_start) %>% arrange(year)
actual <- tables[[i]][2] %>% unlist %>% as.numeric()
pred <- tables[[i]][3] %>% unlist %>% as.numeric()
# 1 calculate R2
R2 <- RSQUARE(actual,pred)
# 2 calculate MAPE
Mape <- MAPE(actual,pred)
#write out processed file
filename_base <- names[[i]] %>% substring(1,nchar(names[[i]])-4)
summary <- c("R2"=R2, "MAPE"=Mape) %>% data.frame()
names(summary) <- c(as.vector(names[[i]]))
alltable <- cbind(alltable, summary)
# write.csv(summary, paste0(filename_base, "_R2.csv"))
}
#cleanup
alltable <- alltable[, -c(1)]
names(alltable) <- names(alltable) %>% substring(1,nchar(.)-23) %>% stri_replace_all_fixed(., " ", "")
cutposition <- names(alltable)[1] %>% gregexpr('/',.) %>% unlist() %>% tail(n=1)
names(alltable) <- sub('_s2_1ha_', '_', names(alltable)) %>% substring(cutposition+1, nchar(.))
alltable <- alltable[,order(colnames(alltable))]
# year-by-year
years <- tables[[1]]$Date %>% as.Date(tables[[i]]$Date, format = "%b %d, %Y") %>% year() %>% unique()
year <- years[years>2018]
summary_table <- vector(mode = "list", length = length(tables))
names(summary_table) <- names(alltable)
for (i in 1:length(tables)) {
annualtable <- data.frame(year = years, R2 = rep(0, length(years)), Mape = rep(0, length(years)))
names(tables[[i]])[1] <- "Date"
tables[[i]]$Date <- as.Date(tables[[i]]$Date, format = "%b %d, %Y")
tables[[i]] <- tables[[i]] %>% filter(Date > analysis_start) %>% arrange(year)
tables[[i]] <- tables[[i]] %>% na.omit()
# no_out <- tables[[i]][[2]] %>% remove_outliers()
# tables[[i]] <- tables[[i]][tables[[i]][[2]] %in% no_out, ]
tables[[i]] <- tables[[i]] %>% group_by(Date) %>% summarise_all(mean)  # check for duplicate data for the same day
for (year in years){ # change this loop to iterate over years
yeardata <- tables[[i]] %>% filter(year(Date) == year)
actual <- yeardata[[2]] %>% unlist %>% as.numeric()
pred <- yeardata[[3]] %>% unlist %>% as.numeric()
# Calculate R2
R2 <- RSQUARE(actual, pred)
# Calculate MAPE
Mape <- MAPE(actual, pred)
# Write out processed file
filename_base <- substring(names(tables[i]), 1, nchar(names(tables[i])) - 4)
annual <- data.frame(year,R2,Mape)
annualtable <- rows_update(annualtable, annual)
}
# Add to yearly totals in the summary table
summary_table[[i]] <- annualtable
}
tables[[1]]
library(stringi)
library(data.table)
library(tidyr)
library(purrr)
library(lubridate)
library(tidyquant)
#Report the R2 statistic for a given pair of fitted and raw values
#working directory
setwd("/Users/brbell01/Google Drive/PHD/CUNY/Research/R/Phenology/raw/s2/1ha/harmonic")
wd <- getwd()
S2_band <- "EVI2" #
analysis_start <- ymd("2019-01-01")
names <- dir(wd, full.names = T, pattern = paste0(S2_band, "_2016_2022_harmonic.csv"))
tables <- names %>% map(read.csv, header=T)
# 1) R-SQUARED error metric -- Coefficient of Determination
RSQUARE <- function(y_actual,y_pred){
cor(y_actual,y_pred)^2
}
# 2) MEAN ABSOLUTE PERCENTAGE ERROR (MAPE)
MAPE <- function(y_actual,y_pred){
mean(abs((y_actual-y_pred)/y_actual))*100
}
#define outlier function
remove_outliers <- function(x, na.rm = TRUE, ...) {
qnt <- quantile(x, probs=c(0.1, 0.9), na.rm = na.rm, ...)
H <- 1.5 * IQR(x, na.rm = na.rm)
y <- x
y[x < (qnt[1] - H)] <- NA
y[x > (qnt[2] + H)] <- NA
y
}
# all years
alltable <- data.frame(matrix(NA, nrow = 2, ncol = 1))
for (i in 1:length(tables)) {
names(tables[[i]])[1] <- "Date"
tables[[i]] <- tables[[i]] %>% na.omit()
# no_out <- tables[[i]][[2]] %>% remove_outliers()
# tables[[i]] <- tables[[i]][tables[[i]][[2]] %in% no_out, ]
tables[[i]] <- tables[[i]] %>% group_by(Date) %>% summarise_all(mean)  # check for duplicate data for the same day.
tables[[i]]$Date <- as.Date(tables[[i]]$Date, format = "%b %d, %Y")
tables[[i]] <- tables[[i]] %>% filter(Date > analysis_start) %>% arrange(Date)
actual <- tables[[i]][2] %>% unlist %>% as.numeric()
pred <- tables[[i]][3] %>% unlist %>% as.numeric()
# 1 calculate R2
R2 <- RSQUARE(actual,pred)
# 2 calculate MAPE
Mape <- MAPE(actual,pred)
#write out processed file
filename_base <- names[[i]] %>% substring(1,nchar(names[[i]])-4)
summary <- c("R2"=R2, "MAPE"=Mape) %>% data.frame()
names(summary) <- c(as.vector(names[[i]]))
alltable <- cbind(alltable, summary)
# write.csv(summary, paste0(filename_base, "_R2.csv"))
}
#cleanup
alltable <- alltable[, -c(1)]
names(alltable) <- names(alltable) %>% substring(1,nchar(.)-23) %>% stri_replace_all_fixed(., " ", "")
cutposition <- names(alltable)[1] %>% gregexpr('/',.) %>% unlist() %>% tail(n=1)
names(alltable) <- sub('_s2_1ha_', '_', names(alltable)) %>% substring(cutposition+1, nchar(.))
alltable <- alltable[,order(colnames(alltable))]
# year-by-year
years <- tables[[1]]$Date %>% as.Date(tables[[i]]$Date, format = "%b %d, %Y") %>% year() %>% unique()
year <- years[years>2018]
summary_table <- vector(mode = "list", length = length(tables))
names(summary_table) <- names(alltable)
for (i in 1:length(tables)) {
annualtable <- data.frame(year = years, R2 = rep(0, length(years)), Mape = rep(0, length(years)))
names(tables[[i]])[1] <- "Date"
tables[[i]]$Date <- as.Date(tables[[i]]$Date, format = "%b %d, %Y")
tables[[i]] <- tables[[i]] %>% filter(Date > analysis_start) %>% arrange(Date)
tables[[i]] <- tables[[i]] %>% na.omit()
# no_out <- tables[[i]][[2]] %>% remove_outliers()
# tables[[i]] <- tables[[i]][tables[[i]][[2]] %in% no_out, ]
tables[[i]] <- tables[[i]] %>% group_by(Date) %>% summarise_all(mean)  # check for duplicate data for the same day
for (year in years){ # change this loop to iterate over years
yeardata <- tables[[i]] %>% filter(year(Date) == year)
actual <- yeardata[[2]] %>% unlist %>% as.numeric()
pred <- yeardata[[3]] %>% unlist %>% as.numeric()
# Calculate R2
R2 <- RSQUARE(actual, pred)
# Calculate MAPE
Mape <- MAPE(actual, pred)
# Write out processed file
filename_base <- substring(names(tables[i]), 1, nchar(names(tables[i])) - 4)
annual <- data.frame(year,R2,Mape)
annualtable <- rows_update(annualtable, annual)
}
# Add to yearly totals in the summary table
summary_table[[i]] <- annualtable
}
#write out summary file
#write.csv(alltable, paste0(wd, "S2_", S2_band, "_harmonic_model_fit_stats.csv"))
# #write out data files
# write.csv(tables[[1]], paste0(wd, "/mata_escura_S2_", S2_band, "_harmonic_model_data.csv"))
# write.csv(tables[[2]], paste0(wd, "/monte_pascoal_S2_", S2_band, "_harmonic_model_data.csv"))
# write.csv(tables[[3]], paste0(wd, "/rio_doce_S2_", S2_band, "_harmonic_model_data.csv"))
# write.csv(tables[[4]], paste0(wd, "/sooretama_S2_", S2_band, "_harmonic_model_data.csv"))
#write out annual data files
write.csv(summary_table[[1]], paste0(wd, "/mata_escura_S2_", S2_band, "_harmonic_model_annual_fit_stats.csv"))
write.csv(summary_table[[2]], paste0(wd, "/monte_pascoal_S2_", S2_band, "_harmonic_model_annual_fit_stats.csv"))
write.csv(summary_table[[3]], paste0(wd, "/rio_doce_S2_", S2_band, "_harmonic_model_annual_fit_stats.csv"))
write.csv(summary_table[[4]], paste0(wd, "/sooretama_S2_", S2_band, "_harmonic_model_annual_fit_stats.csv"))
# Read original time series data files
dir <- dir(paste0(wd, TS_directory), full.names = T, pattern = paste0(S2_band,"_processed.txt"))
#library(rTIMESAT)
library(knitr)
library(tidyr)
library(lubridate)
library(clock)
library(stringr)
library(dplyr)
library(data.table)
library(tibble)
library(purrr)
library(magrittr)
library(padr)
library(ggplot2)
library(scales)
# working directory
setwd("/Users/brbell01/Google Drive/PHD/CUNY/Research/R/Phenology/timesat/")
wd <- getwd()
season_directory <- "/output/third_run/S2/"
TS_directory <- "/formatted_timeseries/third_run/S2/"
# Definitions
S2_int <- 5 # Sentinel-2 repeat interval (days)
S2_start_date <- as.Date("2019-01-01")
S2_band <- "EVI2" # "EVI"  ;  "NDVI" ; "EVI2"
# rTIMESAT functions (doesn't work for me)
# file <- system.file("rio_doce_RVI_seasonality.tpa", package = "rTIMESAT")
# d_tpa <- rTIMESAT::read_tpa(file, t=NULL)
# Read original time series data files
dir <- dir(paste0(wd, TS_directory), full.names = T, pattern = paste0(S2_band,"_processed.txt"))
S2_raw <- dir %>% map(read.csv, header=T)
cutposition <- dir %>% gregexpr('/',.) %>% unlist() %>% tail(n=1)
S2_raw_names <- sub('_processed.txt', '', dir) %>% substring(cutposition+1, nchar(.))
# format original time series data
for (i in 1:length(S2_raw)) {
names(S2_raw[[i]]) <- S2_raw_names[i]
len <- S2_raw[[i]][,1] %>% length()
dates <- seq(as.Date(S2_start_date), by = paste0(S2_int, " days"), length.out=len)
S2_raw[[i]] <- S2_raw[[i]] %>% mutate(Date = dates) %>% relocate(Date)
}
# Read fitted time series and trend data files and append
S2_fitted <- dir(paste0(wd, season_directory), full.names = T, pattern = paste0(S2_band,"_STL_season.txt")) %>% map(fread, sep=" ")
S2_fitted_names <- paste0(S2_raw_names,"_fitted")
S2_trend <- dir(paste0(wd, season_directory), full.names = T, pattern = paste0(S2_band,"_STL_trend.txt")) %>% map(fread, sep=" ")
S2_trend_names <- paste0(S2_raw_names,"_trend")
S2_combined <- list()
for (i in 1:length(S2_fitted)) {
S2_fitted[[i]] <- S2_fitted[[i]] %>% t() %>% data.frame()
names(S2_fitted[[i]]) <- S2_fitted_names[i]
S2_trend[[i]] <- S2_trend[[i]] %>% t() %>% data.frame()
names(S2_trend[[i]]) <- S2_trend_names[i]
len <- S2_fitted[[i]][,1] %>% length()
dates <- seq(S2_start_date, by = paste0(S2_int, " days"), length.out=len)
S2_fitted[[i]] <- S2_fitted[[i]] %>% mutate(Date = dates) %>% relocate(Date)
S2_trend[[i]] <- S2_trend[[i]] %>% mutate(Date = dates) %>% relocate(Date)
S2_combined[[i]] <- right_join(S2_raw[[i]],S2_fitted[[i]],by="Date")
S2_combined[[i]] <- left_join(S2_combined[[i]],S2_trend[[i]],by="Date")
}
# 1) R-SQUARED error metric -- Coefficient of Determination
RSQUARE <- function(y_actual,y_pred){cor(y_actual,y_pred)^2}
# 2) MEAN ABSOLUTE PERCENTAGE ERROR (MAPE)
MAPE <- function(y_actual,y_pred){mean(abs((y_actual-y_pred)/y_actual))*100}
# all years
alltable <- data.frame(matrix(NA, nrow = 2, ncol = 1))
for (i in 1:length(S2_combined)) {
actual <- S2_combined[[i]][2] %>% unlist %>% as.numeric()
pred <- S2_combined[[i]][3] %>% unlist %>% as.numeric()
rm_outliers <- actual>-999
actual <- actual[rm_outliers]
pred <- pred[rm_outliers]
# 1 calculate R2
R2 <- RSQUARE(actual,pred)
# 2 calculate MAPE
Mape <- MAPE(actual,pred)
#write out processed file
filename_base <- names(S2_combined[[i]][2])
summary <- c("R2"=R2, "MAPE"=Mape) %>% data.frame()
names(summary) <- names(S2_combined[[i]][2])
alltable <- cbind(alltable, summary)
write.csv(summary, paste0(wd, season_directory, filename_base, "_R2.csv"))
}
#cleanup
alltable <- alltable[, -c(1)]
names(alltable) <- names(alltable) %>% substring(1,nchar(.)-23) %>% stri_replace_all_fixed(., " ", "")
cutposition <- names(alltable)[1] %>% gregexpr('/',.) %>% unlist() %>% tail(n=1)
names(alltable) <- sub('_s2_1ha_', '_', names(alltable)) %>% substring(cutposition+1, nchar(.))
alltable <- alltable[,order(colnames(alltable))]
# year-by-year
years <- S2_combined[[1]]$Date %>% as.Date(S2_combined[[i]]$Date, format = "%b %d, %Y") %>% year() %>% unique()
summary_table <- vector(mode = "list", length = length(S2_combined))
names(summary_table) <- names(alltable)
for (i in 1:length(S2_combined)) {
annualtable <- data.frame(year = years, R2 = rep(0, length(years)), Mape = rep(0, length(years)))
names(S2_combined[[i]])[1] <- "Date"
S2_combined[[i]]$Date <- as.Date(S2_combined[[i]]$Date, format = "%b %d, %Y")
for (year in years){ # change this loop to iterate over years
yeardata <- S2_combined[[i]] %>% filter(year(Date) == year)
actual <- yeardata[[2]] %>% unlist %>% as.numeric()
pred <- yeardata[[3]] %>% unlist %>% as.numeric()
rm_outliers <- actual>-999
actual <- actual[rm_outliers]
pred <- pred[rm_outliers]
# Calculate R2
R2 <- RSQUARE(actual, pred)
# Calculate MAPE
Mape <- MAPE(actual, pred)
# Write out processed file
filename_base <- substring(names(S2_combined[i]), 1, nchar(names(S2_combined[i])) - 4)
annual <- data.frame(year,R2,Mape)
annualtable <- rows_update(annualtable, annual)
}
# Add to yearly totals in the summary table
summary_table[[i]] <- annualtable
}
#write out summary file
write.csv(alltable, paste0(wd, season_directory, "S2_", S2_band, "_timesat_model_fit_stats.csv"))
#write out fit statistics data file for all study areas
write.csv(S2_combined, paste0(wd, season_directory, "S2_", S2_band, "_timesat_model_combined_data.csv"))
#write out annual data files
write.csv(summary_table[[1]], paste0(wd, "/mata_escura_S2_", S2_band, "_timesat_model_annual_fit_stats.csv"))
write.csv(summary_table[[2]], paste0(wd, "/monte_pascoal_S2_", S2_band, "_timesat_model_annual_fit_stats.csv"))
write.csv(summary_table[[3]], paste0(wd, "/rio_doce_S2_", S2_band, "_timesat_model_annual_fit_stats.csv"))
write.csv(summary_table[[4]], paste0(wd, "/sooretama_S2_", S2_band, "_timesat_model_annual_fit_stats.csv"))
alltable
summary_table
# all years
alltable <- data.frame(matrix(NA, nrow = 2, ncol = 1))
for (i in 1:length(S2_combined)) {
actual <- S2_combined[[i]][2] %>% unlist %>% as.numeric()
pred <- S2_combined[[i]][3] %>% unlist %>% as.numeric()
rm_outliers <- actual>-999
actual <- actual[rm_outliers]
pred <- pred[rm_outliers]
# 1 calculate R2
R2 <- RSQUARE(actual,pred)
# 2 calculate MAPE
Mape <- MAPE(actual,pred)
#write out processed file
filename_base <- names(S2_combined[[i]][2])
summary <- c("R2"=R2, "MAPE"=Mape) %>% data.frame()
names(summary) <- names(S2_combined[[i]][2])
alltable <- cbind(alltable, summary)
write.csv(summary, paste0(wd, season_directory, filename_base, "_R2.csv"))
}
alltable
names(alltable) <- names(alltable) %>% substring(1,nchar(.)-23) %>% stri_replace_all_fixed(., " ", "")
alltable
# all years
alltable <- data.frame(matrix(NA, nrow = 2, ncol = 1))
for (i in 1:length(S2_combined)) {
actual <- S2_combined[[i]][2] %>% unlist %>% as.numeric()
pred <- S2_combined[[i]][3] %>% unlist %>% as.numeric()
rm_outliers <- actual>-999
actual <- actual[rm_outliers]
pred <- pred[rm_outliers]
# 1 calculate R2
R2 <- RSQUARE(actual,pred)
# 2 calculate MAPE
Mape <- MAPE(actual,pred)
#write out processed file
filename_base <- names(S2_combined[[i]][2])
summary <- c("R2"=R2, "MAPE"=Mape) %>% data.frame()
names(summary) <- names(S2_combined[[i]][2])
alltable <- cbind(alltable, summary)
write.csv(summary, paste0(wd, season_directory, filename_base, "_R2.csv"))
}
#cleanup
alltable <- alltable[, -c(1)]
alltable
names(alltable) <- names(alltable) %>% substring(1,nchar(.)-5)
alltable
#write out summary file
write.csv(alltable, paste0(wd, season_directory, "S2_", S2_band, "_timesat_model_fit_stats.csv"))
